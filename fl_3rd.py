# -*- coding: utf-8 -*-
"""FL_3rd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAksLEJ8ns4VvR3LCgC3DqA2gXfWEdOz

## **IMPORT LIBRARIES**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import cv2
import os
from imutils import paths
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

from tensorflow.keras import backend as K

from imblearn.over_sampling import SMOTE

import tensorflow as tf
tf.test.gpu_device_name()

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

!nvidia-smi -q

!nvidia-smi

"""## **DATA LOADING**"""

def load(paths, verbose=-1):
    '''expects images for each class in seperate dir, 
    e.g all digits in 0 class in the directory named 0 '''
    data = list()
    labels = list()
    # for (i, imgpath) in enumerate(paths):
    #     im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)
    #     image = np.array(im_gray).flatten()
    #     label = imgpath.split(os.path.sep)[-2]
    #     data.append(image/255)
    #     labels.append(label)
    #     if verbose > 0 and i > 0 and (i + 1) % verbose == 0:
    #         print("[INFO] processed {}/{}".format(i + 1, len(paths)))
    df = pd.read_csv('/content/final_data_train.csv')
    # df = pd.read_csv('/content/demo1.csv')
    # df.drop(['Unnamed: 0'],axis=1,inplace=True)
    labels = df['islate']
    data = df.drop(['islate'],axis=1)
    labels = labels.values.tolist()
    data = data.values.tolist()

    return data, labels

# from google.colab import drive
# drive.mount('/content/drive/')
# %cd /content/drive/My Drive/trainni/trainingSample

img_path = '/content/Classification_data.csv'
image_paths = list(paths.list_images(img_path))
image_list, label_list = load(image_paths, verbose=10000)
# print(type(image_list))
# print(type(label_list))
lb = LabelBinarizer()
label_list = lb.fit_transform(label_list)
# # sm = SMOTE(random_state=42,sampling_strategy='auto')
# # image_list, label_list = sm.fit_resample(image_list, label_list)
# # print(type(image_list))
# print(type(label_list))
# label_list = label_list.tolist()
# print(type(image_list))
# print(type(label_list))
X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.1, random_state=42,shuffle = True,stratify = label_list)

def create_clients(image_list, label_list, num_clients=10, initial='clients'):
    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]
    data = list(zip(image_list, label_list))
    random.shuffle(data)
    size = len(data)//num_clients
    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]
    assert(len(shards) == len(client_names))
    return {client_names[i] : shards[i] for i in range(len(client_names))}

clients = create_clients(X_train, y_train, num_clients=5, initial='client')

def batch_data(data_shard, bs=320): ##32
    '''Takes in a clients data shard and create a tfds object off it
    args:
        shard: a data, label constituting a client's data shard
        bs:batch size
    return:
        tfds object'''
    data, label = zip(*data_shard)
    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
    return dataset.shuffle(len(label)).batch(bs)

clients_batched = dict()
for (client_name, data) in clients.items():
    clients_batched[client_name] = batch_data(data)
test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))

class SimpleMLP:
    @staticmethod
    def build(shape, classes):
        model = Sequential()
        model.add(Dense(200, input_shape=(shape,)))
        model.add(Activation("relu"))
        model.add(Dense(200))
        model.add(Activation("relu"))
        model.add(Dense(classes))
        model.add(Activation("softmax"))
        return model

"""# **Federated**

## **SGD**
"""

lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = SGD(learning_rate=lr, 
                decay=lr / comms_round, 
                momentum=0.9
               )

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss.append(global_loss)

model_history.history.keys()

import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.plot(global_model_loss)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_adam = pd.DataFrame(global_model_loss)
global_model_loss_adam.to_csv('global_model_loss_SGD.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_SGD.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_SGD.csv')

"""## **ADAM**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = Adam(learning_rate = lr_schedule)   
optimizer = Adam(learning_rate = 0.01, decay = lr/comms_round)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_adam = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_adam.append(global_loss)

import matplotlib.pyplot as plt
from matplotlib import style


plt.style.use('ggplot')
plt.plot(global_model_loss_adam)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_adam = pd.DataFrame(global_model_loss_adam)
global_model_loss_adam.to_csv('global_model_loss_adam.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_adam.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_adam.csv')

"""## **RMSProp**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import RMSprop
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = RMSprop(learning_rate=lr, 
                decay=lr / comms_round, 
                momentum=0.9
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_rmsprop = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_rmsprop.append(global_loss)

import matplotlib.pyplot as plt
from matplotlib import style


plt.style.use('ggplot')
plt.plot(global_model_loss_rmsprop)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_rmsprop = pd.DataFrame(global_model_loss_rmsprop)
global_model_loss_rmsprop.to_csv('global_model_loss_rmsprop.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_rmsprop.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_rmsprop.csv')

"""## **ADAMAX**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adamax
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adamax(learning_rate=lr, 
                decay=lr / comms_round
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_adamax = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_adamax.append(global_loss)

import matplotlib.pyplot as plt
from matplotlib import style


plt.style.use('ggplot')
plt.plot(global_model_loss_adamax)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_adamax = pd.DataFrame(global_model_loss_adamax)
global_model_loss_adamax.to_csv('global_model_loss_adamax.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_adamax.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_adamax.csv')

"""## **FTRL --> not working**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Ftrl
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Ftrl(learning_rate=lr, 
                decay=lr / comms_round
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_ftrl = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_ftrl.append(global_loss)

"""## **NADAM -> not Working**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Nadam
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Nadam(learning_rate=lr, 
                decay=lr / comms_round
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_adamax = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_adamax.append(global_loss)

"""## **Adadelta**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adadelta
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adadelta(learning_rate=lr, 
                decay=lr / comms_round
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_adamax = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_adamax.append(global_loss)

import matplotlib.pyplot as plt
from matplotlib import style


plt.style.use('ggplot')
plt.plot(global_model_loss_adam)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_adamax = pd.DataFrame(global_model_loss_adamax)
global_model_loss_adamax.to_csv('global_model_loss_adadelta.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_adadelta.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_adadelta.csv')

"""## **Adagrad**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adagrad
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adagrad(learning_rate=lr, 
                decay=lr / comms_round
               )    
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round) 
# optimizer = RMSprop(learning_rate = lr_schedule)

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count


def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final



def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad


def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

global_model_loss_adagrad = []
smlp_global = SimpleMLP()
global_model = smlp_global.build(15, 2)
for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()
    client_names= list(clients_batched.keys())
    random.shuffle(client_names)
    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(15, 2)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        local_model.set_weights(global_weights)
        if comm_round is not 99:
            local_model.fit(clients_batched[client], epochs=200, verbose=0)
        else:
            model_history = local_model.fit(clients_batched[client], epochs=200, verbose=1)
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    global_model.set_weights(average_weights)
    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        global_model_loss_adagrad.append(global_loss)

import matplotlib.pyplot as plt
from matplotlib import style


plt.style.use('ggplot')
plt.plot(global_model_loss_adagrad)
plt.xlabel('comm_round')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

global_model_loss_adamax = pd.DataFrame(global_model_loss_adagrad)
global_model_loss_adamax.to_csv('global_model_loss_adagrad.csv')

acc_global_SGD = model_history.history['accuracy']
import matplotlib.pyplot as plt
from matplotlib import style
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(acc_global_SGD)
plt.title('Accuracy of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_accuracy')
# plt.rcParams.update({'font.size': 22})
plt.show()

loss_global_SGD = model_history.history['loss']
plt.style.use('ggplot')
plt.figure(figsize= (12,8))
plt.plot(loss_global_SGD)
plt.title('Loss of global model after 100th Comm_round')
plt.xlabel('Epochs')
plt.ylabel('global_loss')
# plt.rcParams.update({'font.size': 22})
plt.show()

acc_global_SGD = pd.DataFrame(acc_global_SGD)
acc_global_SGD.to_csv('acc_global_adagrad.csv')
loss_global_SGD = pd.DataFrame(loss_global_SGD)
loss_global_SGD.to_csv('loss_global_adagrad.csv')

"""# **Centralized**

## SGD
"""

lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = SGD(learning_rate=lr, 
                decay=lr / comms_round, 
                momentum=0.9
               )

def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    #logits = model.predict(X_test, batch_size=100)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)
##########################
# for comm_round in range(comms_round):
#   for(X_test, Y_test) in test_batched:
#     SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, comm_round)
##########################

SGD_acc_list = []
SGD_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
SGD_acc_list = history.history['accuracy'] 
SGD_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

SGD_acc_list = pd.DataFrame(SGD_acc_list)
SGD_acc_list.to_csv('cen_SGD_acc_list.csv')
SGD_loss_list = pd.DataFrame(SGD_loss_list)
SGD_loss_list.to_csv('cen_SGD_loss_list.csv')
# SGD_acc_list = history.history['accuracy'] 
# SGD_loss_list

"""## **Adam**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adam(learning_rate = 0.01, decay = lr/comms_round)

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay
# lr = 0.01 
# comms_round = 100
# loss='sparse_categorical_crossentropy'
# metrics = ['accuracy']
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round)
# optimizer = Adam(learning_rate=lr, 
#                 decay=lr / comms_round, 
#                 momentum=0.9
#                )  
# optimizer = Adam(learning_rate = lr_schedule)


SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

# history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
Adam_acc_list = []
Adam_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
Adam_acc_list = history.history['accuracy'] 
Adam_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

Adam_acc_list = pd.DataFrame(Adam_acc_list)
Adam_acc_list.to_csv('cen_Adam_acc_list.csv')
Adam_loss_list = pd.DataFrame(Adam_loss_list)
Adam_loss_list.to_csv('cen_Adam_loss_list.csv')
# SGD_acc_list = history.history['accuracy'] 
# SGD_loss_list

"""## **RMSprop**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import RMSprop
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = RMSprop(learning_rate=lr, 
                decay=lr / comms_round, 
                momentum=0.9
               )

# from tensorflow.keras.optimizers import RMSprop
# # from tensorflow.keras.optimizers.schedules import ExponentialDecay
# lr = 0.01 
# comms_round = 100
# loss='sparse_categorical_crossentropy'
# metrics = ['accuracy']
# # lr_schedule = ExponentialDecay(
# #     initial_learning_rate=0.01,
# #     decay_steps=10000,
# #     decay_rate=lr / comms_round)
# # tf.keras.optimizers.RMSprop(
# #     learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,
# #     name='RMSprop', **kwargs
# # )
# optimizer = RMSprop(learning_rate=lr, 
#                 decay=lr / comms_round, 
#                 momentum=0.9
#                )  
# optimizer = Adam(learning_rate = lr_schedule)


SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

# history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
RMS_acc_list = []
RMS_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
RMS_acc_list = history.history['accuracy'] 
RMS_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

RMS_acc_list = pd.DataFrame(RMS_acc_list)
RMS_acc_list.to_csv('cen_RMS_acc_list.csv')
RMS_loss_list = pd.DataFrame(RMS_loss_list)
RMS_loss_list.to_csv('cen_RMS_loss_list.csv')
# SGD_acc_list = history.history['accuracy'] 
# SGD_loss_list

"""## **ADADELTA**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adadelta
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adadelta(learning_rate=lr, 
                decay=lr / comms_round
               )

# from tensorflow.keras.optimizers import Adadelta
# # from tensorflow.keras.optimizers.schedules import ExponentialDecay
# lr = 0.01 
# comms_round = 100
# loss='sparse_categorical_crossentropy'
# metrics = ['accuracy']
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round)
# # tf.keras.optimizers.RMSprop(
# #     learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,
# #     name='RMSprop', **kwargs
# # )
# optimizer = Adadelta(learning_rate=lr_schedule) 
# optimizer = Adam(learning_rate = lr_schedule)


SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

# history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
Adadelta_acc_list = []
Adadelta_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
Adadelta_acc_list = history.history['accuracy'] 
Adadelta_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

Adadelta_acc_list = pd.DataFrame(Adadelta_acc_list)
Adadelta_acc_list.to_csv('cen_Adadelta_acc_list.csv')
Adadelta_loss_list = pd.DataFrame(Adadelta_loss_list)
Adadelta_loss_list.to_csv('cen_Adadelta_loss_list.csv')

"""## **ADAMAX**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adamax
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adamax(learning_rate=lr, 
                decay=lr / comms_round
               )

# from tensorflow.keras.optimizers import Adamax
# # from tensorflow.keras.optimizers.schedules import ExponentialDecay
# lr = 0.01 
# comms_round = 100
# loss='sparse_categorical_crossentropy'
# metrics = ['accuracy']
# lr_schedule = ExponentialDecay(
#     initial_learning_rate=0.01,
#     decay_steps=10000,
#     decay_rate=lr / comms_round)
# # tf.keras.optimizers.Ftrl(
# #     learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,
# #     name='RMSprop', **kwargs
# # )
# optimizer = Adamax(learning_rate=lr_schedule) 
# # optimizer = Adam(learning_rate = lr_schedule)


SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

# history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
Adamax_acc_list = []
Adamax_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
Adamax_acc_list = history.history['accuracy'] 
Adamax_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

Adamax_acc_list = pd.DataFrame(Adamax_acc_list)
Adamax_acc_list.to_csv('cen_Adamax_acc_list.csv')
Adamax_loss_list = pd.DataFrame(Adamax_loss_list)
Adamax_loss_list.to_csv('cen_Adamax_loss_list.csv')

"""## **Adagrad**"""

from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adagrad
lr = 0.01 
comms_round = 100
loss='sparse_categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adagrad(learning_rate=lr, 
                decay=lr / comms_round
               )

SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)
smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(15, 2) 
SGD_model.compile(loss=loss, 
              optimizer=optimizer, 
              metrics=metrics)
history = SGD_model.fit(SGD_dataset, epochs=200, verbose=1)
for(X_test, Y_test) in test_batched:
        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)

# history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
Adagrad_acc_list = []
Adagrad_loss_list = []
# list all data in history
print(history.history.keys())
# summarize history for accuracy
Adagrad_acc_list = history.history['accuracy'] 
Adagrad_loss_list = history.history['loss'] 
plt.plot(history.history['accuracy'])
# plt.plot(history.history['])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()

Adagrad_acc_list = pd.DataFrame(Adagrad_acc_list)
Adagrad_acc_list.to_csv('cen_Adagrad_acc_list.csv')
Adagrad_loss_list = pd.DataFrame(Adagrad_loss_list)
Adagrad_loss_list.to_csv('cen_Adagrad_loss_list.csv')

"""# **PLOTS**

## **SGD**
"""

acc_fl = pd.read_csv('/content/acc_global_SGD.csv')
acc_cen = pd.read_csv('/content/cen_SGD_acc_list.csv')

loss_fl = pd.read_csv('/content/loss_global_SGD.csv')
loss_cen = pd.read_csv('/content/cen_SGD_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

from matplotlib import style

# plt.style.use('ggplot')
plt.style.use('fivethirtyeight')
plt.style.use('seaborn-white')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using SGD: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend(fontsize=20)
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('1-1.png', bbox_inches='tight')

plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using SGD: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend(fontsize=20)
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('1-2.png', bbox_inches='tight')
plt.show()

"""## **RMSprop**"""

acc_fl = pd.read_csv('/content/acc_global_rmsprop.csv')
acc_cen = pd.read_csv('/content/cen_RMS_acc_list.csv')
loss_fl = pd.read_csv('/content/loss_global_rmsprop.csv')
loss_cen = pd.read_csv('/content/cen_RMS_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

from matplotlib import style
# plt.style.use('ggplot')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using RMSprop: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('2-1.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using RMSprop: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('2-2.png', bbox_inches='tight')
plt.show()

"""## **ADAMAX**"""

acc_fl = pd.read_csv('/content/acc_global_adamax.csv')
acc_cen = pd.read_csv('/content/cen_Adamax_acc_list.csv')
loss_fl = pd.read_csv('/content/loss_global_adamax.csv')
loss_cen = pd.read_csv('/content/cen_Adamax_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

# from matplotlib import style
# plt.style.use('ggplot')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using Adamax: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('3-1.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using Adamax: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('3-2.png', bbox_inches='tight')
plt.show()

"""## **Adam**"""

acc_fl = pd.read_csv('/content/acc_global_adam.csv')
acc_cen = pd.read_csv('/content/cen_Adam_acc_list.csv')
loss_fl = pd.read_csv('/content/loss_global_adam.csv')
loss_cen = pd.read_csv('/content/cen_Adam_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

from matplotlib import style
# plt.style.use('ggplot')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using Adam: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('4-1.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using Adam: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('4-2.png', bbox_inches='tight')
plt.show()

"""## **Adagrad**"""

acc_fl = pd.read_csv('/content/acc_global_adagrad.csv')
acc_cen = pd.read_csv('/content/cen_Adagrad_acc_list.csv')
loss_fl = pd.read_csv('/content/loss_global_adagrad.csv')
loss_cen = pd.read_csv('/content/cen_Adagrad_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

from matplotlib import style
# plt.style.use('ggplot')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using Adagrad: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('5-1.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using Adagrad: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('5-2.png', bbox_inches='tight')
plt.show()

"""## **Adadelta**"""

acc_fl = pd.read_csv('/content/acc_global_adadelta.csv')
acc_cen = pd.read_csv('/content/cen_Adadelta_acc_list.csv')
loss_fl = pd.read_csv('/content/loss_global_adadelta.csv')
loss_cen = pd.read_csv('/content/cen_Adadelta_loss_list.csv')

acc_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
acc_cen.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_fl.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_cen.drop(['Unnamed: 0'],axis=1,inplace=True)

from matplotlib import style
# plt.style.use('ggplot')
plt.figure(figsize=(8,6))
plt.plot(acc_fl)
plt.plot(acc_cen)
plt.title('Model Accuracy using Adadelta: Federated vs Centralized')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('6-1.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(8,6))
plt.plot(loss_fl)
plt.plot(loss_cen)
plt.title('Model Loss using Adadelta: Federated vs Centralized')
plt.ylabel('Loss')
plt.xlabel('Epoch')
# plt.legend()
plt.legend(['FL', 'Centralized'],fontsize=20)
plt.savefig('6-2.png', bbox_inches='tight')
plt.show()

"""# **FL Global Model**"""

loss_adadelta = pd.read_csv('/content/global_model_loss_adadelta.csv')
loss_adagrad = pd.read_csv('/content/global_model_loss_adagrad.csv')
loss_adam = pd.read_csv('/content/global_model_loss_adam.csv')
loss_adamax = pd.read_csv('/content/global_model_loss_adamax.csv')
loss_rmsprop = pd.read_csv('/content/global_model_loss_rmsprop.csv')
loss_sgd = pd.read_csv('/content/global_model_loss_SGD.csv')

loss_adadelta.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_adagrad.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_adam.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_adamax.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_rmsprop.drop(['Unnamed: 0'],axis=1,inplace=True)
loss_sgd.drop(['Unnamed: 0'],axis=1,inplace=True)

plt.figure(figsize=(10,6))
plt.plot(loss_sgd)
plt.plot(loss_adadelta)
plt.plot(loss_adagrad)
plt.plot(loss_adam)
plt.plot(loss_adamax)
plt.plot(loss_rmsprop)
# plt.plot(history.history['])
plt.title('Federated Global Model Loss')
plt.ylabel('Loss')
plt.xlabel('Communication Round')
# plt.legend()
plt.legend(['SGD','adadelta', 'adagrad','adam','adamax','rmsprop'],fontsize=15)
plt.savefig('FL_loss.png', bbox_inches='tight')
plt.show()

# plt.plot(SGD_loss_list)
# plt.plot(Adam_loss_list)
# plt.plot(RMS_loss_list)
# plt.plot(Adadelta_loss_list)
# plt.plot(Adamax_loss_list)
# # plt.plot(history.history['])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend()
# plt.legend(['SGD', 'ADAM','RMS','ADADELTA','ADAMAX'])
# plt.show()

